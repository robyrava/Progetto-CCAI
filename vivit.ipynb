{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":722699,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":549864,"modelId":562531}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CREAZIONE E SELEZIONE DEL DATASET","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install allensdk","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Ricerca della Sessione Ottimale","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nfrom pprint import pprint # stampa formattata\n\nimport os\nfrom tqdm import tqdm  \n\nfrom allensdk.core.brain_observatory_cache import BrainObservatoryCache\n\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc # Per il garbage collector\n\n# --- Parametri Richiesti ---\nREQUIRED_STRUCTURES = ['VISp']\nREQUIRED_SESSION_TYPES = ['three_session_A']\nSEARCH_MODE = \"max\"\nEYE_TRACKING = True\nSESSION_FAILED = False\n\n# Inizializza la cache\nboc = BrainObservatoryCache()\n\n# Carica la lista completa di sessioni\nall_sessions = boc.get_ophys_experiments(\n    require_eye_tracking=EYE_TRACKING,\n    include_failed=SESSION_FAILED,\n    session_types=REQUIRED_SESSION_TYPES,\n    targeted_structures=REQUIRED_STRUCTURES    \n)\nprint(f\"Trovate {len(all_sessions)} sessioni 'Session A' totali con eye tracking.\")\n\n# Inizializziamo i contatori per una nuova ricerca\nmax_neuron_count = -1 # Partiamo da -1 per assicurarci che il primo risultato valido venga salvato\nbest_session_id = None\nophys_session_data = None # Caricheremo il data-object solo alla fine\n\nprint(f\"Stato iniziale: Avvio nuova scansione completa.\")\n\n# Imposta la lista delle sessioni da controllare uguale alla lista completa\nremaining_sessions = all_sessions\nprint(f\"Avvio della scansione per {len(remaining_sessions)} sessioni totali...\")\n\n# --- SCANSIONE CON PULIZIA ---\nfor session in remaining_sessions:\n    temp_session_data = None # Resetta per il loop\n    nwb_file_path = None     # Resetta per il loop\n    session_id = session['id']\n            \n    try:\n        print(f\"Tentativo con sessione ID: {session_id}...\")\n        \n        # <<< MODIFICA: Definiamo il percorso del file cache >>>\n        # L'SDK salva i file qui. Dovremo eliminarlo manualmente.\n        nwb_file_path = f\"/kaggle/working/brain_observatory/ophys_experiment_data/{session_id}.nwb\"\n        \n        # Se il file esiste da un run precedente fallito, rimuovilo prima\n        if os.path.exists(nwb_file_path):\n            print(f\"  > Rimuovo file NWB residuo: {nwb_file_path}\")\n            os.remove(nwb_file_path)\n\n        # 1. Scarica i dati (questo è il passaggio che richiede spazio)\n        temp_session_data = boc.get_ophys_experiment_data(ophys_experiment_id=session_id)\n        \n        # 2. Controlla i dati della pupilla (requisito fondamentale)\n        temp_session_data.get_pupil_size()\n        temp_session_data.get_pupil_location()\n        print(f\"  > Dati pupillari validi trovati.\")\n        \n        # 3. Controlla il numero di neuroni\n        num_neurons_list = temp_session_data.get_cell_specimen_ids()\n        current_num_neurons = len(num_neurons_list)\n        print(f\"  > Numero neuroni: {current_num_neurons}\")\n        \n        # 4. Logica di aggiornamento dinamica\n        update = False\n        if SEARCH_MODE == \"max\":\n            if current_num_neurons > max_neuron_count:\n                update = True\n        elif SEARCH_MODE == \"min\":\n            if current_num_neurons > 0 and current_num_neurons < max_neuron_count: \n                update = True\n        \n        if update:\n            print(f\"  > !!! Nuovo record {SEARCH_MODE.upper()}! ({current_num_neurons}). Salvo ID.\")\n            max_neuron_count = current_num_neurons\n            best_session_id = session_id\n            # NON salviamo l'oggetto ophys_session_data, solo il suo ID\n        else:\n            print(f\"  > Non è un record. Scarto.\")\n            \n    except Exception as e:\n        # Se i dati della pupilla o altri metodi falliscono, scarta la sessione\n        print(f\"  > Sessione {session_id} scartata. Errore: {e}\")\n        \n        # Se l'errore è lo spazio su disco, fermati e segnala\n        if \"No space left on device\" in str(e):\n            print(\"!!! ERRORE: Spazio su disco nuovamente esaurito. Interruzione forzata.\")\n            # Non continuare il loop, rilancia l'errore\n            raise e\n        \n        continue # Passa alla sessione successiva\n        \n    finally:\n        # <<< MODIFICA: PULIZIA DEL DISCO >>>\n        # Questo blocco viene eseguito SEMPRE, sia in caso di successo che di errore\n        \n        # Rilascia l'oggetto dalla memoria\n        if temp_session_data is not None:\n            del temp_session_data\n            \n        # Elimina il file NWB dal disco per liberare spazio\n        if nwb_file_path and os.path.exists(nwb_file_path):\n            try:\n                print(f\"  > Pulizia file NWB: {nwb_file_path}\")\n                os.remove(nwb_file_path)\n            except Exception as clean_e:\n                print(f\"  > ERRORE durante la pulizia del file {nwb_file_path}: {clean_e}\")\n        \n        # Forza il garbage collector per liberare memoria\n        gc.collect()\n\n\n# --- Conclusione della Ricerca ---\nif best_session_id is None:\n    print(\"\\nATTENZIONE: Nessuna sessione valida trovata\")\nelse:\n    print(f\"\\n--- SCANSIONE COMPLETATA ---\")\n    print(f\"Modalità di ricerca: {SEARCH_MODE.upper()}\")\n    print(f\"Sessione selezionata (ID): {best_session_id}\")\n    print(f\"Numero di neuroni: {max_neuron_count}\")\n    \n    # <<< MODIFICA: Caricamento finale >>>\n    # Ora, e solo ora, scarichiamo l'oggetto ophys_session_data del vincitore\n    try:\n        print(f\"Caricamento finale dei dati per la sessione {best_session_id}...\")\n        \n        # Pulisci il file se esiste, per sicurezza (es. se era il primo della lista)\n        final_nwb_path = f\"/kaggle/working/brain_observatory/ophys_experiment_data/{best_session_id}.nwb\"\n        if os.path.exists(final_nwb_path):\n             os.remove(final_nwb_path)\n             \n        ophys_session_data = boc.get_ophys_experiment_data(ophys_experiment_id=best_session_id)\n        print(\"Dati della sessione migliore caricati con successo.\")\n    except Exception as e:\n        print(f\"ERRORE CRITICO: Impossibile caricare i dati della sessione migliore {best_session_id}. Errore: {e}\")\n        ophys_session_data = None # Assicura che lo script fallisca dopo\n\nif ophys_session_data is None:\n    print(\"\\nATTENZIONE: Oggetto 'ophys_session_data' non caricato. Il resto dello script fallirà.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_session_id = 501729039","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from allensdk.core.brain_observatory_cache import BrainObservatoryCache\nimport sys\n\n# ID della con il max neuroni trovati\nTARGET_SESSION_ID = best_session_id\n\n# Inizializza la cache\nboc = BrainObservatoryCache()\n\nprint(f\"Caricamento della sessione ID: {TARGET_SESSION_ID}...\")\n\ntry:\n    # Carica direttamente i dati della sessione\n    ophys_session_data = boc.get_ophys_experiment_data(ophys_experiment_id=TARGET_SESSION_ID)\n    \n    # Salviamo l'ID per le celle successive (come i nomi dei run W&B)\n    best_session_id = TARGET_SESSION_ID\n    \n    print(f\"Sessione {best_session_id} caricata con successo.\")\n    \n    # Eseguiamo un controllo di validità\n    ophys_session_data.get_pupil_size()\n    ophys_session_data.get_pupil_location()\n    print(\"Dati pupillari confermati\")\n    \nexcept Exception as e:\n    print(f\"ERRORE: Impossibile caricare la sessione {TARGET_SESSION_ID}. Errore: {e}\")\n    # Fermiamo l'esecuzione se il caricamento fallisce\n    sys.exit(\"Caricamento dati fallito.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualizza gli stimoli\nprint(ophys_session_data.get_stimulus_epoch_table())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"tabella (un DataFrame pandas) che riassume quali stimoli visivi sono stati mostrati al topo e in quali frame\n\n* drifting_gratings: Barre in movimento\n\n\n* natural_movie: un estratto di un film\n\n* spontaneous: Periodi in cui al topo veniva mostrato uno schermo grigio, per misurare l'attività neurale \"a riposo\" o spontanea.\n\n\nIn questo modo sappiamo in quali frame (start e end) andare a cercare i dati che ci interessano (i natural_movie_one e natural_movie_three) e l'attività neurale corrispondente.","metadata":{}},{"cell_type":"code","source":"pprint(ophys_session_data.get_metadata())\nnum_neurons = ophys_session_data.get_cell_specimen_ids()\nprint(f\"\\nnumero nueroni per la sessione: {len(num_neurons)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pre-elaborazione dei Dati di Input","metadata":{}},{"cell_type":"markdown","source":"## Processamento Video (Input Visivo)","metadata":{}},{"cell_type":"markdown","source":"**carichiamo i file video grezzi (i \"template\")**\n**stampiamo le dimensioni (num frame, h frame in pixel, l frame in pixel)**","metadata":{}},{"cell_type":"code","source":"stimulus_template_movie_one = ophys_session_data.get_stimulus_template('natural_movie_one')\nstimulus_template_movie_three = ophys_session_data.get_stimulus_template('natural_movie_three')\nprint(f\"Dimensioni del video 'natural_movie_one': {stimulus_template_movie_one.shape}\")\nprint(f\"Dimensioni del video 'natural_movie_three': {stimulus_template_movie_three.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**I video originali hanno frame di 304x608 pixel ==> troppo grandi per essere processati perchè richiederebbe un'enorme quantità di memoria GPU e tempo di calcolo.\nEseguiamo il downsampling (ricampionamento) per ridurre ogni frame alla dimensione molto più piccola di 36x64 pixel.**","metadata":{}},{"cell_type":"code","source":"from scipy import ndimage\n\nvideos = [stimulus_template_movie_one, stimulus_template_movie_three]\ntarget_height = 36\ntarget_width = 64\n\ndownsampled_videos = []\nfor video in videos:\n    resized_frames = []\n    for frame in video:\n        zoom_factors = (target_height / frame.shape[0], target_width / frame.shape[1])\n        new_frame = ndimage.zoom(frame, zoom_factors, order=1) #order = 1 Specifica il metodo di interpolazione bilineare\n        resized_frames.append(new_frame)\n    \n    downsampled_videos.append(np.array(resized_frames))\n\npprint(downsampled_videos[0].shape)\npprint(downsampled_videos[1].shape) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def segment_data_into_clips(items,pos_num_frames=0, target_frames=140) -> (list): \n    \"\"\"prende un array di dati e lo segmenta in \"clip\" più piccoli tutti della stessa lunghezza.\"\"\"\n    total_frames = items.shape[pos_num_frames]\n    num_items = total_frames // target_frames #scartiamo i frame rimanenti\n    if len(items.shape) == 3: #(frame, altezza, larghezza).\n        return [items[i*target_frames:(i+1)*target_frames, :, :] for i in range(num_items)]\n    elif pos_num_frames==1: # le risposte neurali hanno 2 dimensioni (neuroni, frame).\n        return [items[:, i*target_frames:(i+1)*target_frames] for i in range(num_items)]\n    else:\n        return [items[i*target_frames:(i+1)*target_frames, :] for i in range(num_items)]\n\nclips_movie_one = segment_data_into_clips(downsampled_videos[0])\nclips_movie_three = segment_data_into_clips(downsampled_videos[1])\n\nprint(f\"Numero clip in natural_movie_'one': {len(clips_movie_one)}\")\nprint(f\"Numero clip in natural_movie_'three': {len(clips_movie_three)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**entrambe da 140 frame**","metadata":{}},{"cell_type":"markdown","source":"**sanity check per confermare il numero totale di frame che sono rimasti nel dataset dopo l'operazione di segmentazione effettuata.\nDovremmo avere: 6*140 = 840 e 25*140= 3500**","metadata":{}},{"cell_type":"code","source":"total_frames = 0\nfor clip in clips_movie_one:\n    total_frames += clip.shape[0]\n\nprint(f\"frame totali video 'one' dopo la segmentazione: {total_frames}\")\n\ntotal_frames = 0\nfor clip in clips_movie_three:\n    total_frames += clip.shape[0]\n\nprint(f\"frame totali video'three' dopo la segmentazione: {total_frames}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**effettuiamo un controllo visivo per mostrare l'aspetto di un singolo frame di input**","metadata":{}},{"cell_type":"code","source":"plt.imshow(clips_movie_one[0][50], cmap='gray')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(clips_movie_three[0][50], cmap='gray')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Normalizzazione Video","metadata":{}},{"cell_type":"markdown","source":"**effettuiamo la normalizzazione dei valori dei pixel dei video.\nI modelli \"imparano\" meglio quando i dati di input si trovano in un intervallo di valori piccolo e coerente (come da 0 a 1), piuttosto che in un intervallo ampio (come da 0 a 255).**","metadata":{}},{"cell_type":"code","source":"def calculate_statistics(items):\n    \"\"\"\n    Concatena tutti gli item in un unico array 1D\n    Calcola le statistiche globali (media, std, min, max) per un insieme di clip.\n    \"\"\"\n    \n    all_values = np.concatenate([item.astype(np.float32).ravel() for item in items])\n    mean = np.mean(all_values)\n    std = np.std(all_values)\n    min_val = np.min(all_values)\n    max_val = np.max(all_values)\n\n    return mean, std, min_val, max_val\n\ndef apply_min_max_normalization(items, min_val, max_val, eps=1e-8):\n    \"\"\"\n    Applica la normalizzazione Min-Max (scala 0-1) a una lista di item.\n    Utilizza i valori min e max globali calcolati da calculate_statistics.\n    Formula: (x - min) / (max - min)\n    \"\"\"\n    range_val = max_val - min_val\n    range_safe = np.where(range_val > 0, range_val, eps) #'eps' previene la divisione per zero\n\n    return [(item - min_val) / range_safe for item in items]\n\nmean1, std1, min1, max1 = calculate_statistics(clips_movie_one)\nmean3, std3, min3, max3 = calculate_statistics(clips_movie_three)\n\nprint(\"Video One -> Mean:\", mean1, \"Std:\", std1, \"Min:\", min1, \"Max:\", max1)\nprint(\"Video Three -> Mean:\", mean3, \"Std:\", std3, \"Min:\", min3, \"Max:\", max3)\n\nnormalized_one = apply_min_max_normalization(clips_movie_one, min1,max1)\nnormalized_three = apply_min_max_normalization(clips_movie_three, min3,max3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Rappresentano (luminosità,contrasto,gamma dinamica)\nGamma dinamica ==> pixel completamente neri (0) ; pixel completamente bianchi (255)**","metadata":{}},{"cell_type":"markdown","source":"## Processamento Dati Comportamentali (Pupilla e Corsa)","metadata":{}},{"cell_type":"markdown","source":"**Ora carichiamo i dati comportamentali ==> la posizione del centro della pupilla del topo per ogni singolo frame dell'intero esperimento.**","metadata":{}},{"cell_type":"code","source":"timestamps, pupil_tracking_data = ophys_session_data.get_pupil_location()\nprint(pupil_tracking_data.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 115735: È il numero totale di fotogrammi per l'intera sessione.\n* 2: Rappresenta le coordinate (x, y) del centro della pupilla per ciascuno di quei fotogrammi.","metadata":{}},{"cell_type":"markdown","source":"\n**i video natural_movie_one e natural_movie_three sono stati mostrati 10 volte durante l'esperimento. Tra una ripetizione e l'altra c'erano delle pause (gli \"offset\"). La funzione filter_data serve a estrarre solo i dati esatti di queste 10 ripetizioni, scartando le pause.**","metadata":{}},{"cell_type":"code","source":"def extract_stimulus_trials(item, num_item_frames, offset, pos=None):\n    filtered_list = []\n    for i in range(10):\n        start = i * (num_item_frames + offset)\n        end = start + num_item_frames\n        if pos is not None: # se pos non è None: la forma è [num_neurons, num_frames]\n            filtered_list.append(item[:, start:end])\n        else:\n            filtered_list.append(item[start:end,:])\n\n    if pos is not None:\n        return np.concatenate(filtered_list, axis=1)\n    \n    return np.concatenate(filtered_list, axis=0)\n\npupil_data_movie_one = pupil_tracking_data[38751:47810,:] #38751 (start) e 47810 (end)\npupil_data_movie_one = extract_stimulus_trials(pupil_data_movie_one, 840, 66) #Tra una ripetizione e l'altra ci sono 66 frame di pausa che vanno ignorati\"\nprint(pupil_data_movie_one.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**La funzione ha estratto 10 ripetizioni.\nOgni ripetizione era lunga 840 frame.\nTotale frame: 10 × 840 = 8400 frame.\nIl 2 rappresenta le coordinate (x, y) della pupilla.\nOra pupil_location_one è un array che contiene esattamente i dati di tracciamento oculare per i soli frame in cui il topo stava guardando il \"video one\"**","metadata":{}},{"cell_type":"markdown","source":"**come visto nella tabella degli stimoli, natural_movie_three è stato mostrato in due blocchi separati durante l'esperimento**","metadata":{}},{"cell_type":"code","source":"tmp_pupil_location_three = [pupil_tracking_data[19741:37846,:], pupil_tracking_data[75867:93967,:]]\n\npupil_data_movie_three = []\npupil_data_movie_three.append(extract_stimulus_trials(tmp_pupil_location_three[0], 3500, 121))\npupil_data_movie_three.append(extract_stimulus_trials(tmp_pupil_location_three[1], 3500, 120))\n\nprint(pupil_data_movie_three[0].shape)\nprint(pupil_data_movie_three[1].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5 ripetizioni × 3500 frame/ripetizione = 17.500 frame totali (con 2 coordinate x, y).**","metadata":{}},{"cell_type":"code","source":"# Segmenta il blocco unico di \"movie one\" (che contiene 10 trial)\npupil_location_one_segmented = segment_data_into_clips(pupil_data_movie_one)\n\n# Segmenta il primo blocco di \"movie three\" (5 trial)\npupil_location_three_segmented = segment_data_into_clips(pupil_data_movie_three[0])\n\n# Estende la lista aggiungendo i segmenti del secondo blocco di \"movie three\" (altri 5 trial)\npupil_location_three_segmented.extend(segment_data_into_clips(pupil_data_movie_three[1]))\n\nprint(len(pupil_location_one_segmented))\nprint(len(pupil_location_three_segmented))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"60: Il numero totale di campioni (clip da 140 frame) di dati della pupilla per natural_movie_one.\n\n250: Il numero totale di campioni (clip da 140 frame) di dati della pupilla per natural_movie_three.","metadata":{}},{"cell_type":"markdown","source":"**dividiamo i dati in set di addestramento (train), validazione (validation) e test.**\n**Dobbiamo addestrare il modello sui trial (ripetizioni) iniziali e testarlo sui trial finali. Questo previene il data leakage, ovvero evita che il modello \"veda\" dati futuri durante l'addestramento.**\n\n**70% (Train) / 10% (Validation) / 20% (Test)**","metadata":{}},{"cell_type":"code","source":"pupil_location_one_train = pupil_location_one_segmented[:-12] #Il set di training iniziale è composto dalle prime 48 clip (i primi 8 trial).\npupil_location_one_validation = pupil_location_one_train[-6:] #Il set di validazione è composto dalle ultime 6 clip del set di training (cioè 1 trial\npupil_location_one_train = pupil_location_one_train[:-6] #Il set di addestramento finale è composto dalle 48 clip iniziali meno le 6 di validazione, lasciando 42 clip (i primi 7 trial).\npupil_location_one_test = pupil_location_one_segmented[-12:]  #Il set di test è composto dalle ultime 12 clip (cioè gli ultimi 2 trial)\n\npupil_location_three_train = pupil_location_three_segmented[:-50]\npupil_location_three_validation = pupil_location_three_train[-25:] # 1 trial\npupil_location_three_train = pupil_location_three_train[:-25] \npupil_location_three_test = pupil_location_three_segmented[-50:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"# elementi pupil_location_one_train: {len(pupil_location_one_train)}\")\nprint(f\"# elementi pupil_location_one_validation: {len(pupil_location_one_validation)}\")\nprint(f\"# elementi pupil_location_one_test: {len(pupil_location_one_test)}\")\n\nprint(f\"# elementi pupil_location_three_train: {len(pupil_location_three_train)}\")\nprint(f\"# elementi pupil_location_three_validation: {len(pupil_location_three_validation)}\")\nprint(f\"# elementi pupil_location_three_test: {len(pupil_location_three_test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"locomotion_speed, _ = ophys_session_data.get_running_speed()\n_, pupil_size = ophys_session_data.get_pupil_size()\n\nbehavioral_data = np.column_stack((locomotion_speed, pupil_size))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**behavior è un array dove la prima colonna è la velocità di corsa del topo mentre guarda le clip video e la seconda è la dimensione della pupilla per ogni istante dell'esperimento**","metadata":{}},{"cell_type":"code","source":"# Estrae il blocco grezzo di \"movie one\" (trial + pause)\nbehavior_one = behavioral_data[38751:47810,:]\n\n#Filtra il blocco, tenendo solo i 10 trial (da 840 frame) e scartando le pause (da 66 frame)\nbehavior_one = extract_stimulus_trials(behavior_one, 840, 66)\n\nprint(behavior_one.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 8400: Frame totali (10 trial × 840 frame/trial).\n* 2: Le due colonne di dati (running_speed e pupil_size).\n\nOra abbiamo un array behavior_one  allineato ai frame di pupil_location_one e ai video cropped_one","metadata":{}},{"cell_type":"code","source":"tmp_behavior_three = [behavioral_data[19741:37846,:], behavioral_data[75867:93967,:]]\n\nbehavior_three = []\nbehavior_three.append(extract_stimulus_trials(tmp_behavior_three[0], 3500, 121))\nbehavior_three.append(extract_stimulus_trials(tmp_behavior_three[1], 3500, 120))\n\nprint(behavior_three[0].shape)\nprint(behavior_three[1].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"behavior_one_segmented = segment_data_into_clips(behavior_one)\nbehavior_three_segmented = segment_data_into_clips(behavior_three[0])\nbehavior_three_segmented.extend(segment_data_into_clips(behavior_three[1]))\n\nprint(len(behavior_one_segmented))\nprint(len(behavior_three_segmented))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* 60 clip di dati comportamentali per movie_one.\n* 250 clip di dati comportamentali per movie_three.\n","metadata":{}},{"cell_type":"code","source":"behavior_one_train = behavior_one_segmented[:-12]\nbehavior_one_validation = behavior_one_train[-6:] # 1 trial\nbehavior_one_train = behavior_one_train[:-6]\nbehavior_one_test = behavior_one_segmented[-12:]\n\nbehavior_three_train = behavior_three_segmented[:-50]\nbehavior_three_validation = behavior_three_train[-25:] # 1 trial\nbehavior_three_train = behavior_three_train[:-25] \nbehavior_three_test = behavior_three_segmented[-50:]\n\nprint(f\"# elementi behavior_one_train: {len(behavior_one_train)}\")\nprint(f\"# elementi behavior_one_validation: {len(behavior_one_validation)}\")\nprint(f\"# elementi behavior_one_test: {len(behavior_one_test)}\")\n\nprint(f\"# elementi behavior_three_train: {len(behavior_three_train)}\")\nprint(f\"# elementi behavior_three_validation: {len(behavior_three_validation)}\")\nprint(f\"# elementi behavior_three_test: {len(behavior_three_test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Processamento Dati Neurali (Output/Labels)","metadata":{}},{"cell_type":"markdown","source":"**carichiamo i dati di output (labels) che il nostro modello di intelligenza artificiale dovrà imparare a predire.**\n\n**fluorescence_traces = rappresenta la variazione di fluorescenza ($\\Delta F$) rispetto alla fluorescenza di base ($F$). In termini semplici, ci dice quanto un neurone è attivo in un dato istante rispetto al suo stato di riposo.**","metadata":{}},{"cell_type":"code","source":"# scarica l'attività neurale vera e propria dei 227 neuroni\n_, fluorescence_traces = ophys_session_data.get_corrected_fluorescence_traces()\nprint(fluorescence_traces.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 227: È il numero di neuroni. Ogni riga di questo array è la traccia di attività di un singolo neurone\n* 115735: numero totale di frame dell'intero esperimento (non solo video).\n    * (Corrisponde alla lunghezza dei dati della pupilla e di corsa, visti nelle Celle precedenti).","metadata":{}},{"cell_type":"markdown","source":"**estraiamo l'attività dei 227 neuroni esattamente durante la proiezione del natural_movie_one, scartando le pause.**","metadata":{}},{"cell_type":"code","source":"neural_activity_movie_one = fluorescence_traces[:, 38751:47810]\nneural_activity_movie_one = extract_stimulus_trials(neural_activity_movie_one, 840, 66, \"fluorescence_traces\")\nprint(neural_activity_movie_one.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 227: I 227 neuroni.\n\n* 8400: Il numero totale di frame di attività estratti\n\n    * 10 ripetizioni (trial) × 840 frame/trial = 8400 frame.","metadata":{}},{"cell_type":"code","source":"#Movie Three è stato mostrato in due momenti diversi della sessione\ntmp_neural_activity_three = [fluorescence_traces[:, 19741:37846,], fluorescence_traces[:, 75867:93967]]\n\nneural_activity_movie_three = []\nneural_activity_movie_three.append(extract_stimulus_trials(tmp_neural_activity_three[0], 3500, 121, \"fluorescence_traces\"))\nneural_activity_movie_three.append(extract_stimulus_trials(tmp_neural_activity_three[1], 3500, 120, \"fluorescence_traces\"))\n\nprint(neural_activity_movie_three[0].shape)\nprint(neural_activity_movie_three[1].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"17.500 frame totali / 3.500 frame a video = 5 Trial per blocco\nTot 10 Trial totali per Movie Three","metadata":{}},{"cell_type":"code","source":"# Segmenta l'attività 'fluorescence_traces' di movie_one (8400 frame) in clip da 140\nneural_activity_one_segmented = segment_data_into_clips(neural_activity_movie_one,1)\n\n# Segmenta il primo blocco di 'fluorescence_traces' di movie_three (17500 frame)\nneural_activity_three_segmented = segment_data_into_clips(neural_activity_movie_three[0],1)\n\n# Aggiunge i segmenti del secondo blocco di 'fluorescence_traces' di movie_three (altri 17500 frame)\nneural_activity_three_segmented.extend(segment_data_into_clips(neural_activity_movie_three[1],1))\n\nprint(len(neural_activity_one_segmented))\nprint(len(neural_activity_three_segmented))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Il modello non elabora tutto il video intero in una volta, ma in clip.\n\nMovie One: 8400 frame totali / 140 frame per clip = 60 Clip.\n* Poiché ci sono 10 Trial, ogni Trial è composto da 6 Clip ($60/10=6$).\n\nMovie Three: (17500 + 17500) frame / 140 frame per clip = 250 Clip.\n* Poiché ci sono 10 Trial, ogni Trial è composto da 25 Clip ($250/10=25$).","metadata":{}},{"cell_type":"markdown","source":"**In totale abbiamo 310 campioni (60 + 250) pronti per essere suddivisi in set di addestramento, validazione e test**","metadata":{}},{"cell_type":"code","source":"#SUDDIVISIONE Train / Validation / Test\n\nneural_activity_one_train = neural_activity_one_segmented[:-12]\nneural_activity_one_validation = neural_activity_one_train[-6:] # 1 trial\nneural_activity_one_train = neural_activity_one_train[:-6]\nneural_activity_one_test = neural_activity_one_segmented[-12:]\n\nneural_activity_three_train = neural_activity_three_segmented[:-50]\nneural_activity_three_validation = neural_activity_three_train[-25:] # 1 trial\nneural_activity_three_train = neural_activity_three_train[:-25]\nneural_activity_three_test = neural_activity_three_segmented[-50:]\n\nprint(f\"# elementi neural_activity_one_train: {len(neural_activity_one_train)}\")\nprint(f\"# elementi neural_activity_one_validation: {len(neural_activity_one_validation)}\")\nprint(f\"# elementi neural_activity_one_test: {len(neural_activity_one_test)}\")\n\nprint(f\"# elementi neural_activity_three_train: {len(neural_activity_three_train)}\")\nprint(f\"# elementi neural_activity_three_validation: {len(neural_activity_three_validation)}\")\nprint(f\"# elementi neural_activity_three_test: {len(neural_activity_three_test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Analisi Movie One:**\n* Test: Ultime 12 clip. (12 clip / 6 clip per trial = 2 Trial).\n* Validation: Ultime 6 clip dei rimanenti. (6 / 6 = 1 Trial).\n* Train: Rimanenti 42 clip. (42 / 6 = 7 Trial).\n\nTotale: 7+1+2 = 10 Trial.\n\n**Analisi Movie Three:**\n* Test: Ultime 50 clip. (50 clip / 25 clip per trial = 2 Trial).\n* Validation: Ultime 25 clip dei rimanenti. (25 / 25 = 1 Trial).\n* Train: Rimanenti 175 clip. (175 / 25 = 7 Trial).","metadata":{}},{"cell_type":"markdown","source":"# Preparazione Finale del Dataset","metadata":{}},{"cell_type":"markdown","source":"## Pulizia Dati\n\nrimuoviamo i valori corrotti (`NaN`) dal dataset per evitare crash durante l'addestramento della rete neurale.","metadata":{}},{"cell_type":"code","source":"def impute_missing_data(items,mean = None):\n    \"\"\"\n    Funzione per la pulizia dei dati (NaN e 0) in due modalità.\n    Modalità 1 (se 'mean' è None): Sostituisce i valori NaN (Not a Number) con 0.\n    Modalità 2 (se 'mean' è fornito): Tratta 0 come valore mancante (es. tracking perso) \n    e lo sostituisce con la media fornita (imputazione).\n    \"\"\"\n    items = np.array(items)  # Convert to a NumPy array (if it is not already)\n    if mean:\n         items[items == 0] = mean  \n    else:\n        items[np.isnan(items)] = 0  \n    return items","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Sostituiamo i valori NaN con 0**","metadata":{}},{"cell_type":"code","source":"# TRAIN\nbehavior_one_train = impute_missing_data(behavior_one_train)\nbehavior_three_train = impute_missing_data(behavior_three_train)\n\npupil_location_one_train = impute_missing_data(pupil_location_one_train)\npupil_location_three_train = impute_missing_data(pupil_location_three_train)\n\nneural_activity_one_train = impute_missing_data(neural_activity_one_train)\nneural_activity_three_train = impute_missing_data(neural_activity_three_train)\n\n# VALIDATION\nbehavior_one_validation = impute_missing_data(behavior_one_validation)\nbehavior_three_validation = impute_missing_data(behavior_three_validation)\n\npupil_location_one_validation = impute_missing_data(pupil_location_one_validation)\npupil_location_three_validation = impute_missing_data(pupil_location_three_validation)\n\nneural_activity_one_validation = impute_missing_data(neural_activity_one_validation)\nneural_activity_three_validation = impute_missing_data(neural_activity_three_validation)\n\n# TEST\nbehavior_one_test = impute_missing_data(behavior_one_test)\nbehavior_three_test = impute_missing_data(behavior_three_test)\n\npupil_location_one_test = impute_missing_data(pupil_location_one_test)\npupil_location_three_test = impute_missing_data(pupil_location_three_test)\n\nneural_activity_one_test = impute_missing_data(neural_activity_one_test)\nneural_activity_three_test = impute_missing_data(neural_activity_three_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imputazione e Normalizzazione Finale","metadata":{}},{"cell_type":"markdown","source":"Calcolo parametri di riferimento per la normalizzazione dei dati\n\n**esclusivamente sul Training Set** per evitare il *Data Leakage*. Gli stessi valori verranno poi applicati per scalare Validation e Test set.\n\n1.  **Behavior & Pupil:** Estraiamo `Min` e `Max`. Verranno usati per una **Min-Max Normalization** (range 0-1).\n2.  **Attività Neurale:** Estraiamo `Mean` e `Std`. Verranno usati per la **Z-Score**, più robusta per segnali neurali che possono presentare picchi elevati","metadata":{}},{"cell_type":"code","source":"def calculate_statistics(items):\n    \"\"\"\n    Concatena tutti i campioni in un unico array 1D prima di calcolare \n    le statistiche globali.\n    Calcola le statistiche aggregate (media, deviazione standard, min, max) \n    per un'intera collezione di campioni (es. una lista di clip video).\n\n    Args:\n        items (list): Una lista di array NumPy (es. clip).\n    \"\"\"\n    all_values = np.concatenate([item.astype(np.float32).ravel() for item in items])\n\n    mean = np.mean(all_values)\n    std = np.std(all_values)\n    min_val = np.min(all_values)\n    max_val = np.max(all_values)\n\n    return mean, std, min_val, max_val","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calcola le statistiche SOLO SUL TRAIN SET\nmean_b1_train, _, min_behavior1, max_behavior1 = calculate_statistics(behavior_one_train)\nmean_b3_train, _, min_behavior3, max_behavior3 = calculate_statistics(behavior_three_train)\n\nmean_p1_train, _, min_pupil1, max_pupil1 = calculate_statistics(pupil_location_one_train)\nmean_p3_train, _, min_pupil3, max_pupil3 = calculate_statistics(pupil_location_three_train)\n\nmean_neural_activity1_train, neural_activity_std1, _, _ = calculate_statistics(neural_activity_one_train)\nmean_neural_activity3_train, neural_activity_std3, _, _ = calculate_statistics(neural_activity_three_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"trattiamo tutti i valori 0 (sia quelli originali che quelli ex-NaN) come dati mancanti. \n\nPer i dati comportamentali (es. pupil_location, behavior), uno 0 spesso indica un fallimento del tracciamento (es. l'occhio del topo si è chiuso) e non un valore reale.\n\nUsiamo la funzione fill_missing_values (in Modalità 2) per sostituire tutti questi zeri con la media statistica calcolata **esclusivamente sul set di addestramento** (mean_b1_train) per prevenire il data leakage.\n\n* il modello viene addestrato e validato senza mai \"sbirciare\" informazioni statistiche provenienti dai dati futuri","metadata":{}},{"cell_type":"code","source":"# --- IMPUTAZIONE CON STATISTICHE DEL TRAIN SET ---\n\n# TRAIN\nbehavior_one_train = impute_missing_data(behavior_one_train, mean_b1_train)\nbehavior_three_train = impute_missing_data(behavior_three_train, mean_b3_train)\n\npupil_location_one_train = impute_missing_data(pupil_location_one_train, mean_p1_train)\npupil_location_three_train = impute_missing_data(pupil_location_three_train, mean_p3_train)\n\nneural_activity_one_train = impute_missing_data(neural_activity_one_train, mean_neural_activity1_train)\nneural_activity_three_train = impute_missing_data(neural_activity_three_train, mean_neural_activity3_train)\n\n# VALIDATION\nbehavior_one_validation = impute_missing_data(behavior_one_validation, mean_b1_train)\nbehavior_three_validation = impute_missing_data(behavior_three_validation, mean_b3_train)\n\npupil_location_one_validation = impute_missing_data(pupil_location_one_validation, mean_p1_train)\npupil_location_three_validation = impute_missing_data(pupil_location_three_validation, mean_p3_train)\n\nneural_activity_one_validation = impute_missing_data(neural_activity_one_validation, mean_neural_activity1_train)\n\nneural_activity_three_validation = impute_missing_data(neural_activity_three_validation, mean_neural_activity3_train)\n\n# TEST\nbehavior_one_test = impute_missing_data(behavior_one_test, mean_b1_train)\nbehavior_three_test = impute_missing_data(behavior_three_test, mean_b3_train)\n\npupil_location_one_test = impute_missing_data(pupil_location_one_test, mean_p1_train)\npupil_location_three_test = impute_missing_data(pupil_location_three_test, mean_p3_train)\n\nneural_activity_one_test = impute_missing_data(neural_activity_one_test, mean_neural_activity1_train)\n\nneural_activity_three_test = impute_missing_data(neural_activity_three_test, mean_neural_activity3_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def standardize_neural_activity(items, std):\n    \"\"\"\n    Normalizza una lista di item dividendoli per la deviazione standard (std) globale.\n    Questo metodo è usato specificamente per normalizzare le tracce fluorescence_traces (attività neurale).\n    \"\"\"\n    return [item / std for item in items]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Applicazione della Normalizzazione**\n\nIn questo passaggio trasformiamo i dati grezzi in input pronti per la rete neurale, applicando:\n\n1.  **Min-Max Scaling (per Comportamento e Pupilla):**\n    * Ridimensiona i dati nell'intervallo `[0, 1]`.\n\n2.  **Standardizzazione / Scaling (per Attività Neurale):**\n    * Divide i dati per la deviazione standard (`neural_activity / std`).\n    * Evita che un singolo picco elevato \"schiacci\" tutto il resto del segnale a zero, come accadrebbe con il Min-Max.\n\nI set di **Validation** vengono normalizzati utilizzando le statistiche (`min`, `max`, `std`) calcolate sul **Training Set**. Questo garantisce che il modello veda i nuovi dati attraverso la stessa \"scala di valori\" appresa durante l'addestramento.","metadata":{}},{"cell_type":"code","source":"# --- NORMALIZZAZIONE CON STATISTICHE DEL TRAIN SET ---\n\n# Normalization (Train)\nnormalized_behavior_one = apply_min_max_normalization(behavior_one_train, min_behavior1, max_behavior1)\nnormalized_behavior_three = apply_min_max_normalization(behavior_three_train, min_behavior3, max_behavior3)\nnormalized_pupil_location_one = apply_min_max_normalization(pupil_location_one_train, min_pupil1, max_pupil1)\nnormalized_pupil_location_three = apply_min_max_normalization(pupil_location_three_train, min_pupil3, max_pupil3)\n\nnormalized_neural_activity_one = standardize_neural_activity(neural_activity_one_train, neural_activity_std1)\nnormalized_neural_activity_three = standardize_neural_activity(neural_activity_three_train, neural_activity_std3)\n\n\n# Normalization (Validation)\n# Applichiamo le stesse statistiche (min_behavior1, neural_activity_std1, etc.) usate per il train set\nnormalized_behavior_one_val = apply_min_max_normalization(behavior_one_validation, min_behavior1, max_behavior1)\nnormalized_behavior_three_val = apply_min_max_normalization(behavior_three_validation, min_behavior3, max_behavior3)\nnormalized_pupil_location_one_val = apply_min_max_normalization(pupil_location_one_validation, min_pupil1, max_pupil1)\nnormalized_pupil_location_three_val = apply_min_max_normalization(pupil_location_three_validation, min_pupil3, max_pupil3)\n\nnormalized_neural_activity_one_val = standardize_neural_activity(neural_activity_one_validation, neural_activity_std1)\nnormalized_neural_activity_three_val = standardize_neural_activity(neural_activity_three_validation, neural_activity_std3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Salvataggio dei Dati Elaborati","metadata":{}},{"cell_type":"code","source":"def save_dataset_shards(items, start_idx, output_dir, label=\"\", trials=1):\n    index = start_idx\n    total = trials * len(items)\n    with tqdm(total=total, desc=f\"Saving {label}\", unit=label) as pbar:\n        for i in range(trials):\n            for item in items:\n                filename = f\"{index}.npy\"\n                filepath = os.path.join(output_dir, filename)\n                np.save(filepath, item)\n                index += 1\n                pbar.update(1)\n    return index","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"############################### VIDEOS ################################\noutput_dir = '/kaggle/working/test/data/videos'\nos.makedirs(output_dir, exist_ok=True)\n\ntrials = 2\n\n# Saving \nnext_index = save_dataset_shards(items=normalized_one, start_idx= 0, trials=trials, label=\"normalized_one\", output_dir=output_dir)\n_ = save_dataset_shards(items=normalized_three, start_idx=next_index, trials=trials, label=\"normalized_three\", output_dir=output_dir)\n\n############################### PUPIL LOCATION ################################\noutput_dir = '/kaggle/working/test/data/pupil_center'\nos.makedirs(output_dir, exist_ok=True)\n\n# Normalization of pupil_tracking_data\nnormalized_pupil_location_one = apply_min_max_normalization(pupil_location_one_test, min_pupil1, max_pupil1)\nnormalized_pupil_location_three = apply_min_max_normalization(pupil_location_three_test, min_pupil3, max_pupil3)\n\n# Saving\nnext_index_pupil_center = save_dataset_shards(items=normalized_pupil_location_one, start_idx=0, output_dir=output_dir,label=\"normalized_pupil_location_one\")\n_ = save_dataset_shards(items=normalized_pupil_location_three, start_idx=next_index_pupil_center, output_dir=output_dir,label=\"normalized_pupil_location_three\")\n\n############################### BEHAVIOR ################################\noutput_dir = '/kaggle/working/test/data/behavioral_data'\nos.makedirs(output_dir, exist_ok=True)\n\n# Normalization of behavioral_data\nnormalized_behavior_one = apply_min_max_normalization(behavior_one_test, min_behavior1, max_behavior1)\nnormalized_behavior_three = apply_min_max_normalization(behavior_three_test, min_behavior3, max_behavior3)\n\n# Saving\nnext_index_behavior = save_dataset_shards(items=normalized_behavior_one, start_idx=0,output_dir=output_dir,label=\"normalized_behavior_one\")\n_ = save_dataset_shards(items=normalized_behavior_three, start_idx=next_index_behavior, output_dir=output_dir,label=\"normalized_behavior_three\")\n\n############################### LABELS ################################\noutput_dir = '/kaggle/working/test/data/labels'\nos.makedirs(output_dir, exist_ok=True)\n\n\n# Normalization of fluorescence_traces\nnormalized_neural_activity_one = standardize_neural_activity(neural_activity_one_test, neural_activity_std1)\nnormalized_neural_activity_three = standardize_neural_activity(neural_activity_three_test, neural_activity_std3)\n\n# Saving\nnext_index_neural_activity = save_dataset_shards(items=normalized_neural_activity_one, start_idx=0, output_dir=output_dir,label=\"normalized_neural_activity_one\")\n_ = save_dataset_shards(items=normalized_neural_activity_three, start_idx=next_index_neural_activity ,output_dir=output_dir,label=\"normalized_neural_activity_three\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_dir_videos = '/kaggle/working/train/data/videos'\noutput_dir_behavior = '/kaggle/working/train/data/behavioral_data'\noutput_dir_pupil_center = '/kaggle/working/train/data/pupil_center'\noutput_dir_neural_activity = '/kaggle/working/train/data/labels'\n\noutput_dir_videos_val = '/kaggle/working/validation/data/videos'\noutput_dir_behavior_val = '/kaggle/working/validation/data/behavioral_data'\noutput_dir_pupil_center_val = '/kaggle/working/validation/data/pupil_center'\noutput_dir_neural_activity_val = '/kaggle/working/validation/data/labels'\n\nos.makedirs(output_dir_videos, exist_ok=True)\nos.makedirs(output_dir_behavior, exist_ok=True)\nos.makedirs(output_dir_pupil_center, exist_ok=True)\nos.makedirs(output_dir_neural_activity, exist_ok=True)\n\nos.makedirs(output_dir_videos_val, exist_ok=True)\nos.makedirs(output_dir_behavior_val, exist_ok=True)\nos.makedirs(output_dir_pupil_center_val, exist_ok=True)\nos.makedirs(output_dir_neural_activity_val, exist_ok=True)\n\n# Saving TRAINING\nnext_index = save_dataset_shards(items=normalized_one, start_idx=0, trials=7, label=\"normalized_one\", output_dir=output_dir_videos)\n_ = save_dataset_shards(items=normalized_three, start_idx=next_index, trials=7, label=\"normalized_three\", output_dir=output_dir_videos)\n\nnext_index_behavior = save_dataset_shards(items=normalized_behavior_one, start_idx=0, output_dir=output_dir_behavior, label=\"normalized_behavior_one\")\n_ = save_dataset_shards(items=normalized_behavior_three, start_idx=next_index_behavior, output_dir=output_dir_behavior, label=\"normalized_behavior_three\")\n\nnext_index_pupil_center = save_dataset_shards(items=normalized_pupil_location_one, start_idx=0, output_dir=output_dir_pupil_center, label=\"normalized_pupil_location_one\")\n_ = save_dataset_shards(items=normalized_pupil_location_three, start_idx=next_index_pupil_center, output_dir=output_dir_pupil_center, label=\"normalized_pupil_location_three\")\n\nnext_index_neural_activity = save_dataset_shards(items=normalized_neural_activity_one, start_idx=0, output_dir=output_dir_neural_activity, label=\"normalized_neural_activity_one\")\n_ = save_dataset_shards(items=normalized_neural_activity_three, start_idx=next_index_neural_activity, output_dir=output_dir_neural_activity, label=\"normalized_neural_activity_three\")\n\n# Saving VALIDATION\nnext_index_val = save_dataset_shards(items=normalized_one, start_idx=0, trials=1, label=\"normalized_one_val\", output_dir=output_dir_videos_val)\n_ = save_dataset_shards(items=normalized_three, start_idx=next_index_val, trials=1, label=\"normalized_three_val\", output_dir=output_dir_videos_val)\n\nnext_index_behavior_val = save_dataset_shards(items=normalized_behavior_one_val, start_idx=0, output_dir=output_dir_behavior_val, label=\"normalized_behavior_one_val\")\n_ = save_dataset_shards(items=normalized_behavior_three_val, start_idx=next_index_behavior_val, output_dir=output_dir_behavior_val, label=\"normalized_behavior_three_val\")\n\nnext_index_pupil_center_val = save_dataset_shards(items=normalized_pupil_location_one_val, start_idx=0, output_dir=output_dir_pupil_center_val, label=\"normalized_pupil_location_one_val\")\n_ = save_dataset_shards(items=normalized_pupil_location_three_val, start_idx=next_index_pupil_center_val, output_dir=output_dir_pupil_center_val, label=\"normalized_pupil_location_three_val\")\n\nnext_index_neural_activity_val = save_dataset_shards(items=normalized_neural_activity_one_val, start_idx=0, output_dir=output_dir_neural_activity_val, label=\"normalized_neural_activity_one_val\")\n_ = save_dataset_shards(items=normalized_neural_activity_three_val, start_idx=next_index_neural_activity_val, output_dir=output_dir_neural_activity_val, label=\"normalized_neural_activity_three_val\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creazione classe MouseDataset","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nclass MouseDataset(Dataset):\n    def __init__(self, root_dir):\n        # percorsi per le 4 cartelle di dati\n        self.videos_dir = os.path.join(root_dir, 'videos')\n        self.pupil_dir = os.path.join(root_dir, 'pupil_center')\n        self.behavior_dir = os.path.join(root_dir, 'behavioral_data')\n        self.labels_dir = os.path.join(root_dir, 'labels')\n        \n        # Creo una lista \"master\" di tutti i file ID (es. '0', '1', '2', ...)\n        #    ordinandoli numericamente (es. 1, 2, ... 10, 11)\n        self.file_ids = sorted(\n            [f.replace('.npy', '') for f in os.listdir(self.videos_dir) if f.endswith('.npy')],\n            key=lambda x: int(x)\n        )\n\n    def __len__(self):\n        #ritorna quanti campioni totali ci sono\n        return len(self.file_ids)\n\n    def __getitem__(self, i):\n        #Carica un campione specifico i\n        file_id = self.file_ids[i]\n\n        # Carica i 4 file .npy corrispondenti usando lo STESSO file_id.\n        # Questo garantisce che video, pupilla, comportamento e label\n        # siano perfettamente allineati temporalmente.\n        video = np.load(os.path.join(self.videos_dir, f'{file_id}.npy'))\n        pupil = np.load(os.path.join(self.pupil_dir, f'{file_id}.npy'))\n        behavioral_data = np.load(os.path.join(self.behavior_dir, f'{file_id}.npy'))\n        label = np.load(os.path.join(self.labels_dir, f'{file_id}.npy'))\n\n        #Aggiunge una dimensione \"canale\" al video. \n        # Forma: (140, 36, 64) -> (1, 140, 36, 64) [Canali, Tempo, Altezza, Larghezza]\n        video = video[np.newaxis, ...]\n\n        #Restituisce un dizionario di Tensori PyTorch\n        return {\n            'video': torch.from_numpy(video).float(),\n            'pupil_center': torch.from_numpy(pupil).float().transpose(1,0),\n            'behavioral_data': torch.from_numpy(behavioral_data).float().transpose(1,0),\n            'labels': torch.from_numpy(label).float()\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**creiamo le tre istanze dei set di dati (addestramento, validazione e test) utilizzando la classe MouseDataset.\nQuesti oggetti sono ora pronti per essere passati a un DataLoader di PyTorch**","metadata":{}},{"cell_type":"code","source":"train_set = MouseDataset('/kaggle/working/train/data')\nvalidation_set = MouseDataset('/kaggle/working/validation/data')\ntest_set = MouseDataset('/kaggle/working/test/data')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Configurazione del Modello (ViV1T)","metadata":{}},{"cell_type":"markdown","source":"## Calcolo delle Coordinate Spaziali","metadata":{}},{"cell_type":"code","source":"# Recupera gli ID associati ai 227 neuroni del topo\ncell_specimen_ids = ophys_session_data.get_cell_specimen_ids() \n\n# Per ogni neurone recupera la rispettiva maschera\nroi_masks = ophys_session_data.get_roi_mask_array(cell_specimen_ids=cell_specimen_ids) \nprint(f\"Forma delle maschere ROI: {roi_masks.shape}\")\n\nneuron_centroids = []\nfor i, cell_id in enumerate(cell_specimen_ids):\n    mask = roi_masks[i]\n\n    # Trova tutti i pixel che compongono la sagoma del neurone\n    y_coords, x_coords = np.where(mask)\n\n    if len(x_coords) > 0 and len(y_coords) > 0:\n        # Calcola il centro geometrico (centroide) della sagoma\n        centroid_x = np.mean(x_coords)\n        centroid_y = np.mean(y_coords)\n        neuron_centroids.append([centroid_x, centroid_y])\n    else:\n        # Se la maschera è vuota, aggiunge coordinate nulle\n        neuron_centroids.append([0.0, 0.0])\n\n# Converte la lista di coordinate in un tensore PyTorch\nneuron_centroids = np.array(neuron_centroids, dtype=np.float32)\nneurons_coordinates_tensor = torch.from_numpy(neuron_centroids)\n\nprint(f\"Coordinate dei neuroni:\")\nprint(f\"Forma: {neuron_centroids.shape}\")\nprint(f\"Prime 5 coordinate:\\n {neuron_centroids[:5]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Processiamo le maschere binarie (ROI) per estrarre le coordinate (x, y) del centroide di ciascun neurone e li inseriamo all'interno di un tensore che verrà utilizzato dal componente del modello Readout che utilizzerà queste coordinate per mappare le feature visive estratte dal Core alla specifica posizione fisica del neurone.\n\nQuesto evita che la rete debba apprendere la posizione dei neuroni da zero, accelerando significativamente la convergenza dell'addestramento","metadata":{}},{"cell_type":"markdown","source":"**Il Tensore finale ha 227 righe (una per ogni neurone) e 2 colonne (una per la coordinata x e una per la y).**","metadata":{}},{"cell_type":"code","source":"# Recupera gli ID dei primi tre neuroni\ncids = ophys_session_data.get_cell_specimen_ids()[:3]\nselected_roi_masks = ophys_session_data.get_roi_mask_array(cell_specimen_ids=cids)\n\n# Mostra ogni singola maschera\nf, axes = plt.subplots(1, len(cids)+2, figsize=(15, 3))\nfor ax, roi_mask, cid in zip(axes[:-2], selected_roi_masks, cids):\n    ax.imshow(roi_mask, cmap='gray') \n    ax.set_title('cell %d' % cid)\n\n# Crea una maschera cumulativa di tutte le ROI nell'esperimento\nall_roi_masks = ophys_session_data.get_roi_mask_array()\ncombined_mask = all_roi_masks.max(axis=0)\naxes[-2].imshow(combined_mask, cmap='gray')\naxes[-2].set_title('all ROIs')\n\n# show the movie max projection\nmax_projection = ophys_session_data.get_max_projection()\naxes[-1].imshow(max_projection, cmap='gray')\naxes[-1].set_title('max projection')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* grafico 1-3: Questi primi tre grafici mostrano la forma e la posizione esatta di quei 3 neuroni specifici all'interno del campo visivo 512x512\n* all ROIs: \"appiattisce\" i 227 livelli in un'unica immagine, mostrando la mappa completa di ogni singolo neurone registrato in questo esperimento\n* max projection: Mostra il valore più luminoso che ogni pixel abbia mai raggiunto. È utile per vedere la struttura del tessuto e i vasi sanguigni","metadata":{}},{"cell_type":"markdown","source":"è necessario trasformare le coordinate spaziali dei neuroni dell'Allen Brain Observatory per renderle compatibili con il modello di riferimento SENSORIUM","metadata":{}},{"cell_type":"code","source":"def convert_to_sensorium(roi_coords_tensor, factor=1.24):\n    \"\"\"\n    Converte le coordinate dal sistema in alto a sinistra al sistema Sensorium (alto a destra, con x negativa).\n    \"\"\"\n\n    x_new = roi_coords_tensor[:, 0] * factor   \n    y_new = roi_coords_tensor[:, 1] * factor   \n    \n    x_sensorium = -620 + x_new\n    y_sensorium = -y_new\n    \n    return torch.stack([x_sensorium, y_sensorium], dim=1)\n\nneurons_coordinates_tensor = convert_to_sensorium(neurons_coordinates_tensor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Prime 5 coordinate:\\n {neurons_coordinates_tensor[:5]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Installazione e Caricamento del Modello Base","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!git clone https://github.com/bryanlimy/ViV1T.git\n%cd ViV1T\n!pip install -e .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import importlib.util\nimport numpy as np\nimport argparse\nimport sys\n\nsys.path.insert(0, '/kaggle/input/viv1t/transformers/default/1')\nargs_path = '/kaggle/input/viv1t/transformers/default/1/args.py'\nspec = importlib.util.spec_from_file_location(\"args\", args_path)\nargs = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(args)\n\nargs_dict = args.args_dict\n\nsys.path.append('./src')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install --quiet --force-reinstall \"scipy==1.11.4\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**I pesi shifters e readouts del modello Sensorium sono inutili per noi, perché erano addestrati per un altro topo e altri neuroni.**\n\n**carichiamo nel modello:**\n* Argomenti (args): Definiscono la struttura del Core (il Transformer), garantendo che sia compatibile con i pesi pre-addestrati che caricheremo\n* Coordinate (neuron_coordinates): Definiscono la struttura del Readout","metadata":{}},{"cell_type":"code","source":"from viv1t.model import Model\n\nargs = argparse.Namespace(**args_dict) #per usare la dot-notation\n\nneuron_coordinates = {\n    'A': neurons_coordinates_tensor\n}\n\nviv1t = Model(args, neuron_coordinates=neuron_coordinates)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/input/viv1t/transformers/default/1/model_state.pt\", map_location=args.device, weights_only=False) # modello Sensorium\nstate_dict = checkpoint['model']\n\n# filtriamo i pesi caricando solo i pesi del \"core\" (la parte di elaborazione video)\nfiltered_checkpoint = {}\nfor key, value in state_dict.items():\n    if key.startswith('core.'):\n        filtered_checkpoint[key] = value\n\nviv1t.load_state_dict(filtered_checkpoint, strict=False) # strict=False ==> Carica i pesi del Core e ignora il fatto che mancano i pesi per il nuovo Readout\n\n# Sposta l'intero modello sulla GPU\nviv1t = viv1t.to(args.device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Architettura del modello viv1t**","metadata":{}},{"cell_type":"code","source":"viv1t","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Possiamo dividerlo in 3 parti principali, che lavorano in sequenza.\n\n**core [Video Vision Transformer (ViViT)]**: Si occupa di guardare e capire le clip video.\n\nè composto da\n\n* tokenizer: Divide il video in Tubelet (patch tridimensionali spazio-tempo) e le converte in token  che il Transformer può capire.\n\n* spatial_transformer: analizza i token all'interno di un **singolo** frame per capire le relazioni spaziali (\"cosa c'è nell'immagine?\").\n\n* temporal_transformer: analizza i token **tra** i frame per capire il movimento e le relazioni temporali (\"cosa sta succedendo nel tempo?\").\n\n* rearrange: Riordina i dati di output in una feature map pronta per essere letta.\n\n**MLPShifters**: riceve i dati comportamentali (posizione della pupilla e velocità di corsa) e li usa per modificare spostare l'immagine per compensare il fatto che il topo stava guardando leggermente a destra o a sinistra\n\n**readouts (Gaussian2DReadout)**: Invece di collegare tutti i pixel a tutti i neuroni (che richiederebbe troppi parametri), questo approccio assume che ogni neurone guardi solo una piccola porzione dello schermo. Per pesare l'attenzione in una zona specifica dell'immagine il modello utilizza una funzione Gaussiana \n\n**output_activation: Exponential()**\nApplica una funzione esponenziale all'output per assicurarsi che tutte le previsioni dell'attività neurale siano positive, proprio come i dati reali \"neural_activity\" (la fluorescenza non può essere negativa).","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.cuda.synchronize()\nfree, _ = torch.cuda.mem_get_info()\nprint(f\"Memoria GPU libera: {free / 1e9:.3f} GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Impostazione del Finetuning (PEFT e Trainer)","metadata":{}},{"cell_type":"markdown","source":"la libreria PEFT (usata per il LoRA) è nata per i modelli di linguaggio (come GPT) e si aspetta che il metodo forward del modello accetti parametri standard come input_ids.\n\nIl nostro modello ViV1T, invece, ha un metodo forward personalizzato che richiede parametri specifici come inputs (il video), mouse_id, behaviors, ecc.\n\nQuindi non è possibile utilizzare direttamente get_peft_model a causa di un problema di compatibilità\n\nLa soluzione è creare un Wrapper che adatta l'interfaccia di ViV1T a quella attesa da PEFT.","metadata":{}},{"cell_type":"code","source":"\"\"\"\n%%capture\n!pip install --quiet --upgrade \"transformers\" \"peft\" \"accelerate\"\n!pip install --quiet \"protobuf==3.20.3\"\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wrapper per PEFT (LoRA)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \nfrom peft import get_peft_model, LoraConfig, TaskType\nimport os\nimport warnings\n\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #mostra solo messaggi di errore\nwarnings.filterwarnings('ignore')\n\nclass ViV1TWrapper(nn.Module):\n    \"\"\"Wrapper per adattare ViV1T all'interfaccia PEFT\"\"\"\n    \n    def __init__(self, vivit_model):\n        super().__init__()\n        self.vivit_model = vivit_model\n        \n    def forward(self, input_ids=None, inputs=None, mouse_id=None, behaviors=None, pupil_centers=None, **kwargs):\n        \"\"\"\n        gestisce sia l'interfaccia PEFT che quella originale\n        \"\"\"\n        if inputs is not None:\n            return self.vivit_model(\n                inputs=inputs,\n                mouse_id=mouse_id,\n                behaviors=behaviors,\n                pupil_centers=pupil_centers\n            )\n        else:\n            raise ValueError(\"Missing required parameters for ViV1T model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configurazione del Trainer e Logging","metadata":{}},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working')\n\nprint(\"Current directory:\", os.getcwd())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n\n!pip install huggingface_hub transformers\n!pip install wandb -qqq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nimport os\nfrom huggingface_hub import HfApi\n\nuser_secrets = UserSecretsClient() \n\nwandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_API_KEY\"] = wandb_key\n\nif wandb_key:\n    wandb.login(key=wandb_key)\n    print(\"Accesso a WandB effettuato con successo\")\n\n\nhf_hub_token = user_secrets.get_secret(\"HF_HUB_TOKEN\")\nos.environ[\"HF_HUB_TOKEN\"] = hf_hub_token\n\nif hf_hub_token:\n    print(\"token hugging face hub recuperato con successo\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Wrapper per Trainer di Hugging Face","metadata":{}},{"cell_type":"markdown","source":"Mentre il ViV1TWrapper rende ViV1T compatibile con la libreria PEFT, questo ViViTTrainerWrapper rende il modello (già avvolto da PEFT) compatibile con il Trainer di Hugging Face.\n\nHa due compiti principali:\n\n* Fare da \"ponte\" tra il MouseDataset e il modello.\n* Calcolare la loss del modello.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nfrom types import SimpleNamespace\nimport os\nimport warnings\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nwarnings.filterwarnings('ignore')\n\nclass ViViTTrainerWrapper(nn.Module):\n    def __init__(self, lora_model, mouse_id='A'):\n        super().__init__()\n        self.lora_model = lora_model\n        self.mouse_id = mouse_id\n        self.mse_loss = nn.MSELoss()\n        \n    def forward(self, video, behavioral_data, pupil_center, labels=None, **kwargs): \n        \"\"\"\n        ora accetta gli output del MouseDataset.\n        'labels' è opzionale (None) perché non è presente durante l'inferenza,\n        ma è presente durante il training e la valutazione.\n        \"\"\"\n        \n        # Non c'è più bisogno di kwargs.get()\n        # 'inputs' nel modello interno (lora_model) si aspetta il tensore video\n        predictions, _ = self.lora_model(\n            inputs=video,\n            mouse_id=self.mouse_id,\n            behaviors=behavioral_data,\n            pupil_centers=pupil_center\n        )\n        \n        loss = None\n        if labels is not None:\n            # Allineamento temporale\n            # Il modello riceve 140 frame, ma ne predice solo 66.\n            min_frames = 66 # clip neurali valide, le altre hanno schermo grigio\n            # Estraggo gli ultimi 66 frame dalle etichette (labels)\n            labels_aligned = labels[..., -min_frames:]\n            # Estraggo gli ultimi 66 frame dalle predizioni\n            predictions_aligned = predictions[..., -min_frames:]\n\n            #Calcolo l'errore (MSE) solo su quei 66 frame allineati\n            loss = self.mse_loss(predictions_aligned, labels_aligned) \n        \n        return {\n            \"loss\": loss, #valore scalare dell'errore (da minimizzare).\n            \"logits\": predictions,   #Le predizioni grezze del modello (per calcolare le metriche)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**La classe CustomViViTTrainer estende il Trainer standard della libreria Hugging Face Transformers**\n\n**Lo scopo è sovrascrivere il metodo prediction_step.**","metadata":{}},{"cell_type":"markdown","source":"## Custom Trainer per la Valutazione (Override)\n\nQuesta classe estende il `Trainer` standard di Hugging Face per gestire correttamente l'output personalizzato del nostro modello durante la fase di validazione e test.\n\nPoiché il nostro `ViViTTrainerWrapper` restituisce un dizionario custom (`{'loss': ..., 'logits': ...}`) invece delle tuple standard attese da Hugging Face, è necessario sovrascrivere il metodo **`prediction_step`**.\n\nRitorna la tupla `(loss, logits, labels)` in modo che successivamente la funzione `compute_metrics` riceva i dati corretti.","metadata":{}},{"cell_type":"code","source":"class CustomViViTTrainer(Trainer):\n   \n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    \n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n        \"\"\"\n        Viene chiamato automaticamente durante:\n           - trainer.evaluate() per calcolare metriche sul validation set\n        \"\"\"  \n        # Prepara gli input (es. li sposta sulla GPU)\n        inputs = self._prepare_inputs(inputs) \n\n        # Disattiva il calcolo dei gradienti per risparmiare memoria\n        with torch.no_grad():\n            with self.compute_loss_context_manager(): \n                # Esegue il forward pass chiamando il nostro ViViTTrainerWrapper\n                outputs = model(**inputs)\n                # Recupera la loss e i logits (predizioni) calcolati dal wrapper\n                loss = outputs[\"loss\"]\n                logits = outputs[\"logits\"]\n        \n        if prediction_loss_only: # Se il trainer è configurato per calcolare solo la loss\n            return (loss, None, None)\n\n        # Recupera le labels reali dal batch di input\n        labels = inputs.get('labels')\n        \n        return (loss, logits, labels) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Definizione delle Metriche di Valutazione","metadata":{}},{"cell_type":"markdown","source":"Sarà chiamata dal CustomViViTTrainer alla fine di ogni ogni step di valutazione (epoca) per misurare la performance del modello sul set di validazione.","metadata":{}},{"cell_type":"markdown","source":"## Metrica di Valutazione: Correlazione di Pearson\n\nIn ambito neuroscientifico, l'errore quadratico (MSE) non è sufficiente perchè a noi ci interessa sapere se il modello predice correttamente la **forma d'onda** (il pattern temporale) di quando un neurone si attiva.\n\nLa correlazione di Pearson misura la similarità della forma d'onda.\n\n* 1.0: Le due linee salgono e scendono in perfetta sincronia (predizione perfetta).\n\n* 0.0: Non c'è relazione (il modello tira a indovinare).\n\n* -1.0: Opposto (quando il neurone si attiva, il modello predice che si spegne).\n\n\n*La funzione restituisce:*\n* **`eval_average_single_trial_correlation` (KPI Principale):** La media delle correlazioni di Pearson su tutti i trial. È il valore di riferimento per stabilire la qualità del modello (più alto è meglio).\n* **`eval_single_trial_std` (Stabilità):** La deviazione standard delle correlazioni. Indica quanto le performance oscillano tra un video e l'altro (più basso è meglio, indica coerenza).\n* **`eval_num_examples` (Debug):** Il conteggio dei trial validi effettivamente utilizzati nel calcolo. Utile per verificare che non siano stati scartati troppi campioni a causa di errori numerici (es. `NaN`).","metadata":{}},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred \n    \n    # Allineamento Temporale\n    min_frames = 66 # come nel wrapper della loss valutiamo solo gli ultimi 66 frame, che sono quelli validi.\n    labels_aligned = labels[..., -min_frames:]\n    predictions_aligned = predictions[..., -min_frames:]\n    \n    \n    example_correlations = [] \n    \n    # Itero su ogni clip nel Batch\n    for example_idx in range(labels_aligned.shape[0]):\n\n        # Trasforniamo i dati da (neuroni, tempo) a (tempo, neuroni)\n        # per facilitare il loop successivo.\n        y_true_example = labels_aligned[example_idx].T  \n        y_pred_example = predictions_aligned[example_idx].T\n        \n        correlations = [] \n\n        # All'interno di ogni clip, itero su ogni singolo neurone (da 0 a 226)\n        for neuron in range(y_true_example.shape[1]):\n            # Estrae la serie temporale (66 frame) per UN singolo neurone\n            true_vals = y_true_example[:, neuron]\n            pred_vals = y_pred_example[:, neuron]\n            \n            # Controlla che ci sia varianza (non una linea piatta) per evitare errori  \n            true_var = np.var(true_vals)\n            pred_var = np.var(pred_vals)\n            \n            if true_var > 1e-10 and pred_var > 1e-10:\n                # Calcola la Correlazione di Pearson tra la predizione e la realtà\n                corr = np.corrcoef(true_vals, pred_vals)[0, 1]\n\n                # Aggiunge solo se il risultato è un numero valido\n                if not np.isnan(corr) and np.isfinite(corr):\n                    correlations.append(corr)\n        \n        # Calcola la correlazione media di tutti i neuroni per questo campione\n        mean_corr_example = np.mean(correlations) if correlations else 0.0\n        example_correlations.append(mean_corr_example) \n    \n    # Calcola la media di tutti i campioni nel batch\n    overall_mean = np.mean(example_correlations) if example_correlations else 0.0\n    \n    return {\n        'eval_average_single_trial_correlation': overall_mean,\n        'eval_single_trial_std': np.std(example_correlations) if example_correlations else 0.0,\n        'eval_num_examples': len(example_correlations),\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Strategia di Training 1: Head-Only (Shifter & Readout)","metadata":{}},{"cell_type":"markdown","source":"**Poiché stiamo lavorando su un nuovo set di neuroni, i pesi dei readouts del modello pre-addestrato non sono compatibili**\n\n**Il codice esegue un caricamento parziale: estrae e carica solo i pesi del core (filtrando tramite la stringa \"core.\")** \n\n**e lascia che le nuove head (shifter e readout) vengano inizializzate da zero.**\n\n**L'argomento strict=False permette di caricare i pesi anche se il dizionario non corrisponde perfettamente a tutti i parametri del modello.**\n","metadata":{}},{"cell_type":"code","source":"# Istanzia il modello Viv1T con le coordinate dei neuroni\nviv1t = Model(args, neuron_coordinates=neuron_coordinates)\n# Carica il checkpoint del modello dal percorso specificato\ncheckpoint = torch.load(\"/kaggle/input/viv1t/transformers/default/1/model_state.pt\", map_location=args.device, weights_only=False)\n# Estrae il dizionario degli stati del modello\nstate_dict = checkpoint['model']\n# Inizializza il dizionario filtrato per i pesi del core\nfiltered_checkpoint = {}\n# Filtra soltanto i parametri che appartengono al modulo 'core.'\nfor key, value in state_dict.items():\n    if key.startswith('core.'):\n        filtered_checkpoint[key] = value\n# Carica i pesi filtrati nel modello\nviv1t.load_state_dict(filtered_checkpoint, strict=False)\n# Sposta il modello sulla gpu\nviv1t = viv1t.to(args.device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Congelamento dei Parametri del Core","metadata":{}},{"cell_type":"code","source":"# Congela tutti i parametri del modello (li rende non addestrabili)\nfor param in viv1t.parameters():\n    param.requires_grad = False\n\n# Sblocca i parametri del modulo shifters (li rende addestrabili)\nfor param in viv1t.shifters.parameters():\n    param.requires_grad = True\n\n# Sblocca i parametri del modulo readouts (li rende addestrabili)\nfor param in viv1t.readouts.parameters():\n    param.requires_grad = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configurazione e Avvio del Training","metadata":{}},{"cell_type":"markdown","source":"Questa sezione definisce gli iperparametri e le strategie di addestramento per la fase Head-Only (Shifter + Readout):\n- `num_train_epochs`: numero di volte che il modello vede i dati di training ==> aggiorna i pesi                           dopo ogni batch sia in fase di Learning sia in fase di Inferenza\n- `per_device_train_batch_size` / `per_device_eval_batch_size`: processiamo un solo video alla volta  a causa della memoria limitata di kaggle\n- `learning_rate`: velocità di apprendimento iniziale (1e-3) per la testa del modello.\n- `weight_decay`: Penalizza i pesi del modello se diventano troppo grandi, costringendo il modello a                   imparare feature più semplici ==> riduce overfitting.\n- `fp16`: (Mixed Precision) Invece di usare numeri a 32 bit (float32), usa numeri a 16 bit dove possibile\n- `lr_scheduler_type`: learning rate non rimane fissa a 1e-3. Segue una curva a coseno\n- `eval_strategy`: Alla fine di ogni epoca il modello si ferma e fa il test sul Validation Set.\n- `do_eval`: abilita il ciclo di validazione durante l'addestramento\n- `save_strategy`: salva solo il miglior checkpoint secondo la metrica scelta.\n- `save_total_limit`: limita il numero di checkpoint conservati (1).\n- `output_dir`: cartella dove vengono salvati artefatti e checkpoint dell’esperimento.\n- `metric_for_best_model`: metrica usata per selezionare il checkpoint ottimale (`eval_average_single_trial_correlation`).\n- `greater_is_better`: indica che valori più alti della metrica sono migliori.\n- `load_best_model_at_end`: ricarica il miglior checkpoint al termine del training per valutazioni finali.\n- `logging_steps`: ogni 10 step registra metriche e log (invio a backend).\n- `report_to`: sistema di logging esterno (wandb) per tracking esperimento.\n- `push_to_hub`: disabilita upload automatico su Hugging Face Hub durante il training.\n- `hub_strategy`: strategia di upload (qui non usata perché `push_to_hub=False`).","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    num_train_epochs=40, #30\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    learning_rate=1e-3, \n    weight_decay=0.01,# regolarizzazione L2\n    fp16=True,\n    lr_scheduler_type=\"cosine\",\n\n    # Validazione\n    eval_strategy=\"epoch\", \n    do_eval=True, # abilita esecuzione automatica di compute_metrics sul validation set\n    save_strategy=\"best\",\n    save_total_limit=1,\n\n    # Selezione del modello\n    output_dir=\"./results/Head-40epochs\",\n    metric_for_best_model=\"eval_average_single_trial_correlation\", \n    greater_is_better=True,  \n    load_best_model_at_end=True,  \n    \n    # Logging e Upload\n    logging_steps=10, # ogni 10 step salva metriche e log\n    report_to=\"wandb\",\n\n    # Upload su Hugging Face Hub\n    push_to_hub=False, \n    hub_strategy=\"every_save\",\n    hub_token=hf_hub_token\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"viv1t_wrapped = ViViTTrainerWrapper(viv1t, mouse_id='A')\n\ntrainer_viv1t_wrapped = CustomViViTTrainer(\n    model=viv1t_wrapped,\n    args=training_args,\n    train_dataset=train_set,\n    eval_dataset=validation_set,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Inizializzazione del tracciamento esperimento (Weights & Biases)**\nAvvia una run `wandb` per registrare iperparametri e metadata del training del modello ViViT (solo moduli Shifter e Readout addestrabili)","metadata":{}},{"cell_type":"code","source":"wandb.init(\n    project=\"Brain-Encoding-ViV1T-Trainings\",\n    entity=\"c-h-r-o-ll-o16198-8-universit-catania\",\n    name=\"ViViT_Shifter_Readout_Only-V1\",\n    config={\n        # Parametri di addestramento standard\n        \"learning_rate\": training_args.learning_rate,  \n        \"epochs\": training_args.num_train_epochs,\n        \"batch_size\": training_args.per_device_train_batch_size,\n        \"lr_scheduler_type\": training_args.lr_scheduler_type,\n        \"warmup_steps\": training_args.warmup_steps,           \n        \"weight_decay\": training_args.weight_decay,           \n        \"max_grad_norm\": training_args.max_grad_norm,          \n        \n        # Informazioni sul modello\n        \"model_architecture\": \"ViViT_Shifter_Readout_Only\",\n        \"optimizer_type\": \"AdamW_custom_lr\",\n        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n        \"fp16\": training_args.fp16,\n        \"save_strategy\": training_args.save_strategy,\n    }\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_viv1t_wrapped.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# miglior valore della metrica di monitoraggio registrato durante l'intero ciclo di addestramento.\ntrainer_viv1t_wrapped.state.best_metric","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Salvataggio (Head-Only)","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\napi = HfApi()\n\napi.create_repo(\n    repo_id=\"robyrava/vivit-brain-encoding\",\n    token=hf_hub_token,\n    private=False,  \n    exist_ok=True  \n)\n\napi.upload_folder(\n    folder_path=\"./results/Head-40epochs/\",\n    repo_id=\"robyrava/vivit-brain-encoding\",\n    path_in_repo=\"experiments/Head-40epochs/\",\n    token=hf_hub_token\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CARICAMENTO DA HUGGINGFACE**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\n\nour_checkpoint_path = hf_hub_download(\n    repo_id=\"robyrava/vivit-brain-encoding\",\n    filename=\"experiments/Head-40epochs/checkpoint-5642/model.safetensors\",  \n    token=hf_hub_token\n)\n\nstate_dict = load_file(our_checkpoint_path)\n\nviv1t = Model(args, neuron_coordinates=neuron_coordinates)\nviv1t = viv1t.to(args.device)\nviv1t_wrapped = ViViTTrainerWrapper(viv1t, mouse_id='A')\n\nviv1t_wrapped.load_state_dict(state_dict)\n\ntrainer_viv1t_wrapped = CustomViViTTrainer(\n    model=viv1t_wrapped,\n    args=training_args,\n    train_dataset=train_set,\n    eval_dataset=validation_set,\n    compute_metrics=compute_metrics,\n)\n\n# Controllo rapido (sanity check) sulla validazione\nvalidation_results = trainer_viv1t_wrapped.evaluate(eval_dataset=validation_set)\nprint(f\"Risultati (eval_single_trial_correlation) sul validation set: {validation_results['eval_average_single_trial_correlation']}\")\n\n# Valutazione del modello sul test set\ntest_results = trainer_viv1t_wrapped.evaluate(eval_dataset=test_set)\nprint(f\"Risultati (eval_single_trial_correlation) sul test set: {test_results['eval_average_single_trial_correlation']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Commento Risultati (Head-Only)**\n\n**Validazione:** correlazione ≈ 0.026, leggermente sopra la baseline (≈0.0235); il modello coglie una minima struttura del segnale neurale sui trial di validazione.\n**Test:** correlazione ≈ -0.00077 (≈0), non generalizza; indica overfitting o scarsa robustezza del mapping neurale finale.\n**Sintesi:** la testa (shifter + readout) apprende pattern specifici del set di validazione ma non trasferisce informazione ai trial non visti.\n","metadata":{}},{"cell_type":"markdown","source":"# Strategia di Training 2: LoRA + Head","metadata":{}},{"cell_type":"code","source":"CURRENT_RANK = 8  #8, 16, o 32\nTRAIN_EPOCHS = 40","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**target_modules**: dove inserire le matrici LoRA all'interno del Transformer.\n\n* fused_linear: gestisce le proiezioni Query, Key, Value dell'Attenzione. È il posto standard per LoRA.\n\n* attn_out: La proiezione in uscita dal blocco di attenzione.\n\n* ff_out.2: La parte finale del blocco Feed-Forward (MLP).","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom transformers import TrainingArguments\nfrom huggingface_hub import HfApi\nimport wandb\nimport os\n\nLORA_CONFIGS = {\n    8: {\n        \"alpha\": 16,\n        #\"target_modules\": [\"fused_linear\"],\n        \"target_modules\": [\"fused_linear\", \"attn_out\", \"ff_out.2\"],\n        #\"learning_rate\": 0.001,\n        \"learning_rate\": 0.0036, # molto più alto perchè con r=8 ci sono pochi parametri da addestrare\n        #\"scheduler\": \"cosine\",\n        \"scheduler\": \"linear\", # più adatto a learning_rate alti\n        #\"description\": \"fused_only_cosine\"\n        \"description\": \"all_layers_linear\"\n    },\n    16: {\n        \"alpha\": 32,\n        #\"target_modules\": [\"fused_linear\", \"attn_out\", \"ff_out.2\"], # \"All\" layers\n        \"target_modules\": [\"fused_linear\"],\n        \"learning_rate\": 0.001,\n        \"scheduler\": \"linear\",\n        #\"description\": \"all_layers_linear\"\n        \"description\": \"fused_linear\"\n    },\n    32: {\n        \"alpha\": 64,\n        #\"target_modules\": [\"fused_linear\", \"attn_out\", \"ff_out.2\"], # \"All\" layers\n        \"target_modules\": [\"fused_linear\"], # Solo \"Fused\"\n        #\"learning_rate\": 0.0036,\n        \"learning_rate\": 0.001,\n        #\"scheduler\": \"linear\",\n        \"scheduler\": \"cosine\",\n        #\"description\": \"all_layers_linear_highLR\"\n        \"description\": \"fused_only_cosine\"\n    }\n}\n\n# Recupero della configurazione corrente\ncurrent_config = LORA_CONFIGS[CURRENT_RANK]\n\n\nrun_name = f\"ViViT_LoRA-{CURRENT_RANK}_{current_config['description']}\"\noutput_dir = f\"./results/{run_name}\"\nrepo_path = f\"experiments/{run_name}\"\n\nprint(f\"--- Starting Experiment: Rank {CURRENT_RANK} ---\")\nprint(f\"Configuration: {current_config}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PREPARAZIONE DEL MODELLO E LORA","metadata":{}},{"cell_type":"code","source":"peft_config = LoraConfig(\n    r=CURRENT_RANK,\n    lora_alpha=current_config[\"alpha\"],\n    lora_dropout=0.1, #durante l'addestramento, spegne casualmente il 10% dei neuroni nelle matrici LoRA ad ogni passaggio.\n    bias=\"none\", #alleniamo solo i pesi (w) ignorando i bias\n    task_type=TaskType.FEATURE_EXTRACTION, #perchè è un modello che prende un input (video) e ne estrae una rappresentazione numerica (le feature) che poi userò per predire l'attività neurale\n    target_modules=current_config[\"target_modules\"] #quali layer del modello originale devono essere \"avvolti\" dalle matrici LoRA\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Caricamento del Modello Base\nviv1t = Model(args, neuron_coordinates=neuron_coordinates)\n\n# Caricamento pesi pre-addestrati (solo core)\ncheckpoint = torch.load(\"/kaggle/input/viv1t/transformers/default/1/model_state.pt\", map_location=args.device, weights_only=False)\nstate_dict = checkpoint['model']\n\nfiltered_checkpoint = {}\nfor key, value in state_dict.items():\n    if key.startswith('core.'):\n        filtered_checkpoint[key] = value\n\nviv1t.load_state_dict(filtered_checkpoint, strict=False)\nviv1t = viv1t.to(args.device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Applicazione di LoRA\nlora_model = get_peft_model(ViV1TWrapper(viv1t), peft_config) \nlora_model.print_trainable_parameters()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scongelamento delle head (Shifter & Readout)\n# È necessario rendere trainabili anche le parti non-LoRA specifiche per il topo 'A'\nfor param in lora_model.base_model.model.vivit_model.shifters.parameters():\n    param.requires_grad = True\n    \nfor param in lora_model.base_model.model.vivit_model.readouts.parameters():\n    param.requires_grad = True\n    \nprint(\"Trainable parameters after unfreezing heads:\")\nlora_model.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TRAINING SETUP","metadata":{}},{"cell_type":"code","source":"# Argomenti di addestramento\ntraining_args = TrainingArguments(\n    num_train_epochs=TRAIN_EPOCHS,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    \n    # Parametri dinamici dal dizionario\n    learning_rate=current_config[\"learning_rate\"], \n    lr_scheduler_type=current_config[\"scheduler\"],\n    \n    weight_decay=0.01,\n    fp16=True,\n\n    # Validazione e salvataggio\n    eval_strategy=\"epoch\",\n    save_total_limit=1,\n    do_eval=True, \n    save_strategy=\"best\",\n    \n    # Selezione del modello\n    output_dir=output_dir,\n    metric_for_best_model=\"eval_average_single_trial_correlation\", \n    greater_is_better=True,  # Ci importa che la Correlazione sia alta\n    load_best_model_at_end=True,  \n    \n    # Logging\n    logging_steps=10, \n    report_to=\"wandb\",\n    run_name=run_name, \n\n    # Hub\n    push_to_hub=False,  \n    hub_strategy=\"every_save\",\n    hub_token=hf_hub_token\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inizializzazione Trainer\nlora_viv1t_wrapped = ViViTTrainerWrapper(lora_model, mouse_id='A')\n\ntrainer = CustomViViTTrainer(\n    model=lora_viv1t_wrapped,\n    args=training_args,\n    train_dataset=train_set,\n    eval_dataset=validation_set,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inizializzazione WandB\nwandb.init(\n    project=\"Brain-Encoding-ViV1T-Trainings\",\n    entity=\"c-h-r-o-ll-o16198-8-universit-catania\",\n    name=run_name,\n    config={\n        \"rank\": CURRENT_RANK,\n        \"alpha\": current_config[\"alpha\"],\n        \"target_modules\": current_config[\"target_modules\"],\n        \"learning_rate\": current_config[\"learning_rate\"],\n        \"scheduler\": current_config[\"scheduler\"],\n        \"epochs\": TRAIN_EPOCHS,\n        \"model_architecture\": f\"ViViT_LoRA-{CURRENT_RANK}\",\n    }\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ESECUZIONE","metadata":{}},{"cell_type":"code","source":"# Avvio Training\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Upload su Hugging Face Hub\napi = HfApi()\napi.create_repo(\n    repo_id=\"robyrava/vivit-brain-encoding\",\n    token=hf_hub_token,\n    private=False,  \n    exist_ok=True  \n)\n\nprint(f\"Uploading results to {repo_path}...\")\napi.upload_folder(\n    folder_path=output_dir, \n    repo_id=\"robyrava/vivit-brain-encoding\",\n    path_in_repo=repo_path, \n    token=hf_hub_token\n)\n\nprint(f\"Best Validation Metric: {trainer.state.best_metric}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Valutazione Finale (usa il modello 'best' caricato automaticamente alla fine del training)\nval_results = trainer.evaluate(eval_dataset=validation_set)\nprint(f\"Final Validation Score: {val_results['eval_average_single_trial_correlation']}\")\n\ntest_results = trainer.evaluate(eval_dataset=test_set)\nprint(f\"Final Test Score: {test_results['eval_average_single_trial_correlation']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CONFRONTO RISULTATI CON LA BASELINE","metadata":{}},{"cell_type":"code","source":"baseline_result = 0.0235345645802377","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_result(test_results):\n    ratio = test_results/baseline_result\n    if ratio < 1:\n        print(\"Il modello riesce a spiegare meno dei dati stessi\")\n    elif ratio > 1:\n        print(f\"Il modello sta spiegando non solo la media, ma anche qualcosa in più (migliore di {ratio:.2f}) volte\")\n    else:\n        print(\"Sono uguali\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Risultati Head-Only","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nour_checkpoint_path = hf_hub_download(\n    repo_id=\"robyrava/vivit-brain-encoding\",\n    filename=\"experiments/Head-V1/checkpoint-217/model.safetensors\",\n    token=hf_hub_token\n)\n\nstate_dict = load_file(our_checkpoint_path)\n\nviv1t = Model(args, neuron_coordinates=neuron_coordinates)\nviv1t = viv1t.to(args.device)\nviv1t_wrapped = ViViTTrainerWrapper(viv1t, mouse_id='A')\n\nviv1t_wrapped.load_state_dict(state_dict)\n\n\ntrainer_viv1t_wrapped = CustomViViTTrainer(\n    model=viv1t_wrapped,\n    args=training_args,\n    train_dataset=train_set,\n    eval_dataset=validation_set,\n    compute_metrics=compute_metrics,\n)\n\ntest_results = trainer_viv1t_wrapped.evaluate(eval_dataset=test_set)['eval_average_single_trial_correlation']\nprint(f\"Risultati sul test set: {test_results}\")\nprint(f\"Risulato baseline: {baseline_result}\")\n\nprint_result(test_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Risultati LoRA (r=8, 16, 32)","metadata":{}},{"cell_type":"code","source":"#vedere su huggingface\ncheckpoint_filename = \"experiments/ViViT_LoRA-8_all_layers_linear/checkpoint-8029/model.safetensors\" \nNUM_CHECKPOINT = 8029","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\nprint(f\"Tentativo di download del file: {checkpoint_filename}\")\n\n#Ricaricamento dello Stato del Modello\ntry:\n    our_checkpoint_path = hf_hub_download(\n        repo_id=\"robyrava/vivit-brain-encoding\",\n        filename=checkpoint_filename,\n        token=hf_hub_token\n    )\n\n    state_dict = load_file(our_checkpoint_path)\n\n    # Re-inizializzazione del Modello Base (Senza PEFT)\n    viv1t = Model(args, neuron_coordinates=neuron_coordinates)\n    \n    # Re-inizializzazione della configurazione LoRA\n    lora_config = LoraConfig(\n        r=CURRENT_RANK,\n        lora_alpha=current_config[\"alpha\"],\n        lora_dropout=0.1,\n        bias=\"none\",\n        task_type=TaskType.FEATURE_EXTRACTION,\n        target_modules=current_config[\"target_modules\"]\n    )\n    \n    # Re-applicazione di LoRA al modello\n    lora_model = get_peft_model(ViV1TWrapper(viv1t), lora_config) \n    \n    # Il caricamento dei pesi deve essere fatto sul modello wrapped\n    lora_viv1t_wrapped = ViViTTrainerWrapper(lora_model, mouse_id='A')\n    lora_viv1t_wrapped.load_state_dict(state_dict)\n\n# Valutazione\n\n    # Il trainer va re-inizializzato con il modello ricaricato e i training_args\n    # Riusa training_args e compute_metrics definiti nelle celle precedenti\n    trainer_lora_viv1t_wrapped = CustomViViTTrainer(\n        model=lora_viv1t_wrapped,\n        args=training_args, \n        train_dataset=train_set,\n        eval_dataset=validation_set,\n        compute_metrics=compute_metrics,\n    )\n\n    # Valutazione del modello sui dati di test\n    test_results = trainer_lora_viv1t_wrapped.evaluate(eval_dataset=test_set)['eval_average_single_trial_correlation']\n    \n    print(f\"\\n--- Risultati LoRA Rank {CURRENT_RANK} (Checkpoint Ricaricato: {NUM_CHECKPOINT}) ---\")\n    print(f\"Risultati (eval_single_trial_correlation) sul test set: {test_results}\")\n    \n    print(f\"Risultato baseline: {baseline_result}\")\n    print_result(test_results)\n\nexcept Exception as e:\n    print(f\"\\nERRORE nel caricamento del checkpoint {NUM_CHECKPOINT}: {e}\")\n    print(\"Controlla che il numero del checkpoint e la configurazione del Rank (CURRENT_RANK) siano corretti.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}